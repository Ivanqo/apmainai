# app.py

import os
import asyncio
import time
import logging
from collections import Counter
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence
from torchcrf import CRF
from transformers import AutoModel, AutoTokenizer
import pandas as pd
from fastapi import FastAPI, Request
from pydantic import BaseModel
import uvicorn
import boto3
import requests

best_checkpoint = "ner_model/checkpoint-15000"
os.makedirs(best_checkpoint, exist_ok=True)

# –°—é–¥–∞ –≤–ø–∏—à–∏ —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å–≤–æ–∏ —Ñ–∞–π–ª—ã –∏–∑ Object Storage
BASE_URL = "https://storage.yandexcloud.net/model-inf"
MODEL_FILES = [
    "pytorch_model.bin",
    "vocab.txt",
    "tokenizer.json",
    "tokenizer_config.json",
    "special_tokens_map.json"
]

def download_from_bucket(best_checkpoint):
    logging.info(f"–ü—Ä–æ–≤–µ—Ä—è—é –∏ —Å–æ–∑–¥–∞—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é: {best_checkpoint}")
    os.makedirs(best_checkpoint, exist_ok=True)

    missing_files = []
    for file in MODEL_FILES:
        local_path = os.path.join(best_checkpoint, file)
        if os.path.exists(local_path) and os.path.getsize(local_path) > 0:
            logging.info(f"–§–∞–π–ª —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {file}")
            continue
        missing_files.append(file)

    if not missing_files:
        logging.info("–í—Å–µ —Ñ–∞–π–ª—ã —É–∂–µ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç, —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è ‚úÖ")
        return

    for file in missing_files:
        url = f"{BASE_URL}/{file}"
        logging.info(f"–ù–∞—á–∏–Ω–∞—é —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ {file} –∏–∑ {url}")
        try:
            r = requests.get(url, stream=True, timeout=60)
            r.raise_for_status()

            total_size = int(r.headers.get("content-length", 0))
            logging.info(f"–†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞ {file}: {total_size} –±–∞–π—Ç")

            local_path = os.path.join(best_checkpoint, file)
            with open(local_path, "wb") as f:
                downloaded = 0
                start_time = time.time()
                for chunk in r.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
                        downloaded += len(chunk)
                        if total_size > 0:
                            percent = (downloaded / total_size) * 100
                            elapsed = time.time() - start_time
                            speed = downloaded / (elapsed + 1e-6) / 1024
                            remaining = (total_size - downloaded) / (speed * 1024 + 1e-6)
                            if downloaded // total_size * 100 % 10 == 0:
                                logging.info(
                                    f"{file}: {percent:.2f}% —Å–∫–∞—á–∞–Ω–æ, "
                                    f"{downloaded}/{total_size} –±–∞–π—Ç, —Å–∫–æ—Ä–æ—Å—Ç—å {speed:.2f} KB/s, "
                                    f"–æ—Å—Ç–∞–ª–æ—Å—å ~{remaining:.1f} —Å–µ–∫"
                                )

            if os.path.getsize(local_path) == 0:
                logging.error(f"–§–∞–π–ª {local_path} –ø—É—Å—Ç –ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏!")
            else:
                logging.info(f"–§–∞–π–ª {file} —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω ‚úÖ")

        except Exception as e:
            logging.exception(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ {file}: {e}")



# -------------------------
# –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
# -------------------------
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

# -------------------------
# –ö–ª–∞—Å—Å –º–æ–¥–µ–ª–∏ ‚Äî –ë–ª–æ–∫ 1
# -------------------------
class NERWithCRF(nn.Module):
    def __init__(self, model_name, num_labels, lstm_hidden=256, lstm_layers=2, dropout_prob=0.3):
        super().__init__()

        # –ó–∞–≥—Ä—É–∂–∞–µ–º —ç–Ω–∫–æ–¥–µ—Ä
        self.encoder = AutoModel.from_pretrained(
            model_name,
            use_safetensors=True,
            output_hidden_states=True
        )

        self.hidden_size = self.encoder.config.hidden_size
        self.num_hidden_layers = self.encoder.config.num_hidden_layers + 1  # –≤–∫–ª—é—á–∞—è embeddings

        # –ë—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 4 —Å–ª–æ—è
        self.num_used_layers = 4
        self.layer_weights = nn.Parameter(torch.ones(self.num_used_layers) / self.num_used_layers)

        # LSTM
        self.lstm = nn.LSTM(
            input_size=self.hidden_size,
            hidden_size=lstm_hidden,
            num_layers=lstm_layers,
            batch_first=True,
            bidirectional=True,
            dropout=dropout_prob if lstm_layers > 1 else 0
        )

        self.layer_norm = nn.LayerNorm(lstm_hidden * 2)
        self.dropout = nn.Dropout(dropout_prob)

        # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä
        self.classifier = nn.Sequential(
            nn.Linear(lstm_hidden * 2, 256),
            nn.GELU(),
            nn.Dropout(dropout_prob),
            nn.Linear(256, num_labels)
        )

        # CRF —Å–ª–æ–π
        self.crf = CRF(num_labels, batch_first=True)

        # –í–µ—Å–∞ –¥–ª—è –∫–ª–∞—Å—Å–æ–≤ (–µ—Å–ª–∏ –±—É–¥—É—Ç –Ω—É–∂–Ω—ã –ø–æ–∑–∂–µ)
        self.class_weights = nn.Parameter(torch.ones(num_labels), requires_grad=False)

    def forward(self, input_ids, attention_mask=None, labels=None):
        device = input_ids.device
        attention_mask = attention_mask.to(device)
        if labels is not None:
            labels = labels.to(device)

        outputs = self.encoder(
            input_ids=input_ids,
            attention_mask=attention_mask,
            output_hidden_states=True
        )

        hidden_states = outputs.hidden_states  # tuple –∏–∑ num_hidden_layers —Å–ª–æ—ë–≤
        last_layers = torch.stack(hidden_states[-self.num_used_layers:])  # (4, batch, seq, hidden)

        # –í–∑–≤–µ—à–∏–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 4 —Å–ª–æ—è
        weighted_layers = (last_layers * self.layer_weights.view(-1, 1, 1, 1)).sum(0)

        # LSTM
        lengths = attention_mask.sum(dim=1)  # –¥–ª–∏–Ω—ã –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
        packed_input = pack_padded_sequence(weighted_layers, lengths.cpu(), batch_first=True, enforce_sorted=False)
        packed_output, _ = self.lstm(packed_input)

        # –ü–∞–¥–¥–∏–Ω–≥ –¥–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã –≤—Ö–æ–¥–∞
        lstm_out, _ = pad_packed_sequence(
            packed_output,
            batch_first=True,
            total_length=attention_mask.size(1)  # –∫–ª—é—á–µ–≤–æ–π –º–æ–º–µ–Ω—Ç
        )

        lstm_out = self.layer_norm(lstm_out)
        lstm_out = self.dropout(lstm_out)

        emissions = self.classifier(lstm_out)
        mask = attention_mask.bool()

        if labels is not None:
            labels_clean = labels.clone()
            labels_clean[labels == -100] = 0

            crf_loss = -self.crf(emissions, labels_clean, mask=mask, reduction="mean")
            macro_f1_loss = self.macro_f1_loss(self.crf.decode(emissions, mask=mask), labels_clean, mask)

            loss = crf_loss + 0.3 * macro_f1_loss
            return {"loss": loss, "logits": emissions}
        else:
            predictions = self.crf.decode(emissions, mask=mask)
            return {"logits": emissions, "predictions": predictions}


    def macro_f1_loss(self, preds, labels, mask):
        eps = 1e-8
        macro_f1 = 0
        num_classes = self.classifier[-1].out_features

        preds_tensor = torch.zeros_like(labels, device=labels.device)
        for i, seq in enumerate(preds):
            preds_tensor[i, :len(seq)] = torch.tensor(seq, device=labels.device)

        for cls in range(1, num_classes):  # –ø—Ä–æ–ø—É—Å–∫–∞–µ–º O
            tp = ((preds_tensor == cls) & (labels == cls) & mask).sum().float()
            fp = ((preds_tensor == cls) & (labels != cls) & mask).sum().float()
            fn = ((preds_tensor != cls) & (labels == cls) & mask).sum().float()
            f1 = 2 * tp / (2 * tp + fp + fn + eps)
            macro_f1 += (1 - f1)

        return macro_f1 / (num_classes - 1)

# -------------------------
# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ ‚Äî –ë–ª–æ–∫ 2
# -------------------------
label_list = ["O", "B-TYPE", "I-TYPE", "B-BRAND", "I-BRAND", 
              "B-VOLUME", "I-VOLUME", "B-PERCENT", "I-PERCENT"]
label2id = {label: idx for idx, label in enumerate(label_list)}
id2label = {idx: label for label, idx in label2id.items()}

model = None
tokenizer = None
DEVICE = None

d# -------------------------
# –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ ‚Äî –ë–ª–æ–∫ 2
# -------------------------
label_list = ["O", "B-TYPE", "I-TYPE", "B-BRAND", "I-BRAND", 
              "B-VOLUME", "I-VOLUME", "B-PERCENT", "I-PERCENT"]
label2id = {label: idx for idx, label in enumerate(label_list)}
id2label = {idx: label for label, idx in label2id.items()}

model = None
tokenizer = None
DEVICE = None

def load_model_sync():
    global model, tokenizer, DEVICE
    try:
        logger.info(f"–ü—Ä–æ–≤–µ—Ä—è—é —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏: {best_checkpoint}")
        if os.path.exists(best_checkpoint):
            files = os.listdir(best_checkpoint)
            logger.info(f"–§–∞–π–ª—ã –≤ {best_checkpoint}: {files}")
        else:
            logger.error(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è {best_checkpoint} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç!")

        # –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç—É –∂–µ –º–æ–¥–µ–ª—å, —á—Ç–æ –∏ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏
        model_checkpoint = "DeepPavlov/rubert-base-cased-conversational"
        
        logger.info("–ó–∞–≥—Ä—É–∂–∞—é —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä...")
        tokenizer = AutoTokenizer.from_pretrained(best_checkpoint)
        logger.info("–¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω ‚úÖ")

        logger.info("–°–æ–∑–¥–∞—é –º–æ–¥–µ–ª—å —Å —Ç–µ–º–∏ –∂–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏...")
        model = NERWithCRF(
            model_name=model_checkpoint,  # –¢–û–ß–ù–û –¢–ê–ö –ñ–ï –ö–ê–ö –ü–†–ò –û–ë–£–ß–ï–ù–ò–ò
            num_labels=len(label_list),
        )
        logger.info("–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏ —Å–æ–∑–¥–∞–Ω–∞ ‚úÖ")

        model_path = os.path.join(best_checkpoint, "pytorch_model.bin")
        if not os.path.exists(model_path):
            logger.error(f"–§–∞–π–ª –≤–µ—Å–æ–≤ –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {model_path}")
            return

        logger.info(f"–ó–∞–≥—Ä—É–∂–∞—é –≤–µ—Å–∞ –º–æ–¥–µ–ª–∏ –∏–∑ {model_path}")
        state_dict = torch.load(model_path, map_location="cpu")
        
        # –í–∞–∂–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: —Å—Ä–∞–≤–Ω–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
        model_state_keys = set(model.state_dict().keys())
        loaded_state_keys = set(state_dict.keys())
        
        if model_state_keys != loaded_state_keys:
            logger.warning(f"–†–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–µ –≤ –∫–ª—é—á–∞—Ö –º–æ–¥–µ–ª–∏!")
            missing = model_state_keys - loaded_state_keys
            extra = loaded_state_keys - model_state_keys
            if missing:
                logger.warning(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –∫–ª—é—á–∏: {missing}")
            if extra:
                logger.warning(f"–õ–∏—à–Ω–∏–µ –∫–ª—é—á–∏: {extra}")
        
        model.load_state_dict(state_dict, strict=False)  # strict=False –¥–ª—è —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏
        logger.info("–í–µ—Å–∞ –º–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã ‚úÖ")

        DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
        model.to(DEVICE)
        model.eval()
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
        logger.info(f"–ú–æ–¥–µ–ª—å –ø–µ—Ä–µ–º–µ—â–µ–Ω–∞ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {DEVICE}")
        logger.info(f"–ü–µ—Ä–≤—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä –º–æ–¥–µ–ª–∏ –Ω–∞: {next(model.parameters()).device}")
        
        logger.info("–ú–æ–¥–µ–ª—å —Å CRF —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ üöÄ")

    except Exception as e:
        logger.exception(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: {e}")
        raise

# -------------------------
# –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ ‚Äî –ë–ª–æ–∫ 3
# -------------------------
def aggregate_labels(subtok_labels: list) -> str:
    """
    –ì–∏–±—Ä–∏–¥–Ω–∞—è –∞–≥—Ä–µ–≥–∞—Ü–∏—è:
    1. –ï—Å–ª–∏ –≤—Å–µ –º–µ—Ç–∫–∏ –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ ‚Üí –≤–µ—Ä–Ω—É—Ç—å –∏—Ö.
    2. –ï—Å–ª–∏ –º–µ—Ç–∫–∏ —Ç–æ–ª—å–∫–æ B-/I- –æ–¥–Ω–æ–≥–æ —Ç–∏–ø–∞ ‚Üí –≤–µ—Ä–Ω—É—Ç—å B- —ç—Ç–æ–≥–æ —Ç–∏–ø–∞.
    3. –ï—Å–ª–∏ —Å–º–µ—Å—å —Å O ‚Üí –≤–µ—Ä–Ω—É—Ç—å –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç—É—é (majority).
    """
    if len(set(subtok_labels)) == 1:
        return subtok_labels[0]

    # –£–±–∏—Ä–∞–µ–º O –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω–∞ –æ–¥–∏–Ω —Ç–∏–ø
    non_o = [l for l in subtok_labels if l != "O"]
    if non_o:
        types = {l.split("-", 1)[-1] for l in non_o}
        if len(types) == 1:
            # –æ–¥–∏–Ω —Ç–∏–ø, –Ω–æ —Ä–∞–∑–Ω—ã–π –ø—Ä–µ—Ñ–∏–∫—Å ‚Üí –Ω–∞—á–∏–Ω–∞–µ–º —Å B-
            return "B-" + list(types)[0]

    # fallback ‚Üí majority
    return Counter(subtok_labels).most_common(1)[0][0]


with open("/app/brands_all.txt", "r", encoding="utf-8") as f:
    BRANDS = [line.strip().lower() for line in f if line.strip()]
logger.info(f"{BRANDS}")
BRANDS += ["coca-cola", "pepsi", "fanta", "sprite", "lipton", "lay's", "pringles", "kitkat", "oreo", "milka", "–ø—Ä–æ—Å—Ç–æ–∫–≤–∞—à–∏–Ω–æ", "–¥–æ–º–∏–∫ –≤ –¥–µ—Ä–µ–≤–Ω–µ", "valio", "danone", "–¥–æ–±—Ä—ã–π", "rich", "j7", "bonaqua", "—Å–≤—è—Ç–æ–π –∏—Å—Ç–æ—á–Ω–∏–∫"]

VOLUME_UNITS = {"–≥", "–∫–≥", "–ª", "–º–ª", "—à—Ç", "–≥—Ä", "–≥—Ä–∞–º–º", "–∫–∏–ª–æ–≥—Ä–∞–º–º"}
PERCENT_UNITS = {"%", "–ø—Ä–æ—Ü", "–ø—Ä–æ—Ü–µ–Ω—Ç"}

def validate_entity(entity_type: str, text: str) -> bool:
    text = text.strip().lower()
    if not text or len(text) == 1:
        return False

    if entity_type == "TYPE":
        return len(text) >= 3 and not text.isdigit()
    if entity_type == "BRAND":
        return text in BRANDS
    if entity_type == "VOLUME":
        return any(unit in text for unit in VOLUME_UNITS) and any(char.isdigit() for char in text)
    if entity_type == "PERCENT":
        return any(unit in text for unit in PERCENT_UNITS) and any(char.isdigit() for char in text)
    return False

def postprocess_annotations(text: str, raw_annotations: list) -> list:
    if not raw_annotations:
        return [(0, len(text), "O")]  # –≤–µ—Å—å —Ç–µ–∫—Å—Ç –∫–∞–∫ O, –µ—Å–ª–∏ –ø—É—Å—Ç–æ

    merged = []
    prev_label = None
    prev_type = None
    prev_end = -1  # –∏–Ω–¥–µ–∫—Å –∫–æ–Ω—Ü–∞ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —Å–ø–∞–Ω–∞

    for start_char, end_char, label in raw_annotations:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ "–¥—ã—Ä—É" –º–µ–∂–¥—É –ø—Ä–µ–¥—ã–¥—É—â–∏–º –∏ —Ç–µ–∫—É—â–∏–º —Å–ø–∞–Ω–æ–º
        if start_char > prev_end + 1:
            gap_start = prev_end + 1
            gap_end = start_char - 1
            gap_text = text[gap_start:gap_end + 1].strip()

            # 1. –ï—Å–ª–∏ –¥—ã—Ä–∞ –æ—á–µ–Ω—å –º–∞–ª–µ–Ω—å–∫–∞—è (< 3 —Å–∏–º–≤–æ–ª–æ–≤) ‚Üí –ø—Ä–∏—Å–æ–µ–¥–∏–Ω—è–µ–º –∫ —Å–æ—Å–µ–¥—è–º
            if len(gap_text) <= 2:
                if merged and prev_label and prev_label != "O":
                    # —Ä–∞—Å—à–∏—Ä—è–µ–º –ø—Ä–µ–¥—ã–¥—É—â—É—é —Å—É—â–Ω–æ—Å—Ç—å
                    last_start, last_end, last_label = merged[-1]
                    merged[-1] = (last_start, gap_end, last_label)
                else:
                    # –µ—Å–ª–∏ –ø–æ—Å–ª–µ –±—É–¥–µ—Ç —Å—É—â–Ω–æ—Å—Ç—å ‚Üí —Ä–∞—Å—à–∏—Ä–∏–º –µ—ë
                    start_char = gap_start
            # 2. –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç –≤ –¥—ã—Ä–∫–µ —Å–∞–º —è–≤–ª—è–µ—Ç—Å—è –≤–∞–ª–∏–¥–Ω–æ–π —Å—É—â–Ω–æ—Å—Ç—å—é
            elif gap_text:
                for etype in ["BRAND", "VOLUME", "PERCENT", "TYPE"]:
                    if validate_entity(etype, gap_text):
                        merged.append((gap_start, gap_end, f"B-{etype}"))
                        break
                else:
                    # 3. –ò–Ω–∞—á–µ –¥—ã—Ä–∞ ‚Üí "O"
                    merged.append((gap_start, gap_end, "O"))

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–µ–∫—É—â–∏–π —Ç–æ–∫–µ–Ω
        if label == "O":
            merged.append((start_char, end_char, "O"))
            prev_label = None
            prev_type = None
            prev_end = end_char
            continue

        current_type = label.split("-", 1)[1]

        # –°–∫–ª–µ–π–∫–∞ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π
        if prev_label and (prev_label.startswith("B-") or prev_label.startswith("I-")):
            if current_type == prev_type and start_char == prev_end + 1:
                merged.append((start_char, end_char, f"I-{current_type}"))
                prev_label = f"I-{current_type}"
                prev_end = end_char
                continue

        merged.append((start_char, end_char, label))
        prev_label = label
        prev_type = current_type
        prev_end = end_char

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ö–≤–æ—Å—Ç –ø–æ—Å–ª–µ –ø–æ—Å–ª–µ–¥–Ω–µ–π —Å—É—â–Ω–æ—Å—Ç–∏
    if prev_end < len(text) - 1:
        gap_start = prev_end + 1
        gap_text = text[gap_start:].strip()
        if gap_text:
            # –µ—Å–ª–∏ –æ—Å—Ç–∞—Ç–æ–∫ ‚Äî –∏–∑–≤–µ—Å—Ç–Ω–∞—è —Å—É—â–Ω–æ—Å—Ç—å
            added = False
            for etype in ["BRAND", "VOLUME", "PERCENT", "TYPE"]:
                if validate_entity(etype, gap_text):
                    merged.append((gap_start, len(text) - 1, f"B-{etype}"))
                    added = True
                    break
            if not added:
                merged.append((gap_start, len(text) - 1, "O"))

    return merged

def predict_annotations(
    text: str,
    keep_O: bool = True,
    fix_i_without_b: bool = True,
) -> list:

    if not text.strip():
        return []

    encoding = tokenizer(
        text,
        return_offsets_mapping=True,
        is_split_into_words=False,
        return_special_tokens_mask=True,
        truncation=True,
        return_tensors="pt"
    )

    device = next(model.parameters()).device
    input_ids = encoding["input_ids"].to(device)
    attention_mask = encoding["attention_mask"].to(device)
    offsets = encoding["offset_mapping"].squeeze(0)
    word_ids = encoding.word_ids(batch_index=0)

    model.eval()
    with torch.no_grad():
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        preds_seq = outputs["predictions"][0]

    word_to_token_idxs = {}
    for tok_idx, w_id in enumerate(word_ids):
        if w_id is None:
            continue
        word_to_token_idxs.setdefault(w_id, []).append(tok_idx)

    raw_annotations = []
    prev_word_label = None
    n_words = max(word_to_token_idxs.keys()) + 1 if word_to_token_idxs else 0

    for word_id in range(n_words):
        token_idxs = word_to_token_idxs.get(word_id, [])
        if not token_idxs:
            continue

        start_char = int(offsets[token_idxs[0]][0].item())
        end_char = int(offsets[token_idxs[-1]][1].item())

        subtok_labels = [id2label[preds_seq[tok_idx]] for tok_idx in token_idxs]
        label = aggregate_labels(subtok_labels)

        # –§–∏–∫—Å–∏–º "I-" –±–µ–∑ "B-"
        if label.startswith("I-") and fix_i_without_b:
            typ = label.split("-", 1)[1]
            prev_typ = None
            if prev_word_label and prev_word_label != "O":
                prev_typ = prev_word_label.split("-", 1)[1]
            if prev_word_label is None or prev_word_label == "O" or prev_typ != typ:
                label = "B-" + typ

        if label == "O" and not keep_O:
            prev_word_label = "O"
            continue

        raw_annotations.append((start_char, end_char, label))
        prev_word_label = label

    return postprocess_annotations(text, raw_annotations)

# -------------------------
# FastAPI –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ
# -------------------------
app = FastAPI(title="NER BIO API", version="1.0")

class PredictRequest(BaseModel):
    input: str

@app.on_event("startup")
async def startup_event():
    loop = asyncio.get_event_loop()
    logger.info("–ó–∞–ø—É—Å–∫ –ø—Ä–æ—Ü–µ—Å—Å–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏...")

    # –°–Ω–∞—á–∞–ª–∞ —Å–∫–∞—á–∏–≤–∞–µ–º —Ñ–∞–π–ª—ã
    await loop.run_in_executor(None, download_from_bucket, best_checkpoint)

    # –ü–æ—Ç–æ–º –∑–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
    await loop.run_in_executor(None, load_model_sync)


@app.post("/api/predict")
async def predict(req: PredictRequest, request: Request):
    start_time = time.time()
    text = req.input
    if not text.strip():
        return []

    loop = asyncio.get_event_loop()
    annotations = await loop.run_in_executor(None, predict_annotations, text)

    elapsed = time.time() - start_time
    logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∑–∞–Ω—è–ª–∞ {elapsed:.4f} —Å–µ–∫—É–Ω–¥")
    return [
        {"start_index": start, "end_index": end, "entity": label}
        for start, end, label in annotations
        if label != "O"
    ]


@app.get("/health")
async def health():
    return {"status": "ok... Mb)"}
